Computer vision is a lucrative field of artificical intelligence and there is a lot of research going on in this field \cite{Szegedy_2016_CVPR} \cite{wright2010sparse} \cite{vedaldi2010vlfeat}. Computer vision is an interdisciplinary study that extracts interpretations of the world from pictures or sequence of pictures and can automate tasks that the human visual system can do. It has even surpassed human visual abilities in some areas. It is being used in a lot of different real-world applications including medical imaging, retail, optical character recognition, 3D model building, surveillance, motion capture, automotive safety, machine safety, etc \cite{cv_book}. Computer vision is not new field. It has been around for 1950s and its first commercial use in recognition of handwritten or typed text took place in 1970s. The emergence of computer vision today is due to a number of factors including: affordable and easily accessible computer power, large amount of photos and videos from mobile phones and development of algorithms like convolutional neural network optimized for computer vision taking advantage of software and hardware capabilities. There are many different tasks \ref{fig:computer_vision_tasks} in the field of computer vision including Object Detection, Object Localization, Image Classification, Image captioning, Instance Segmentation, Semantic Segmentation, etc. \textit{Image classification} is the process of taking an image as an input and outputting a class or a probability that the input image is of a specific class. For example classifying cats or dogs from images. \textit{Object classification and localization} just adjust the output labels from the image classification algorithm to make our model learn the object category as well as the object's location in the image. The output layer gives 4 more numbers which are the center location (x and y coordinates) of the object and the width and height of the bounding box in the image. For \textit{multiple objects detection and localization} the only difference is that we want our model to be able to classify and locate all objects in an image, not just one. Its most common applications are in surveillance, security and self-driving vehicles. We can find a lot of pre-trained models for object detection today such as Multibox, RCNN, Fast RCNN, Mask RCNN, YOLO, etc. \textit{Semantic segmentation} is a process that seeks to recognize or undertstand what is in an image at pixel level or simply image classification at pixel level. It does so by labeling each pixel in the image with a category label. For example labels could be cat, grass, tree, and sky in the Figure \ref{fig:computer_vision_tasks}. However, it does not differentiate among instances. Robot vision, autonomous driving and medical imaging are some areas where it is very useful. \textit{Instance segmentation} is the task of identifying and distinguishing each separate object of interest that appears in an image.


\begin{figure}[H]
  \centering
  \includegraphics[scale=0.35]{images/Chapter1/computer_vision_tasks.png}
  \caption{Different tasks in Computer Vision \cite{cv_tasks}}
  \label{fig:computer_vision_tasks}
\end{figure}

% ImageNet, the competion

% The convolutional neural network (CNN) is a class of deep learning neural networks. CNNs represent a huge breakthrough in image recognition. They’re most commonly used to analyze visual imagery and are frequently working behind the scenes in image classification. They can be found at the core of everything from Facebook’s photo tagging to self-driving cars. They’re working hard behind the scenes in everything from healthcare to security.

% Computer vision
% Scene classification, places
% talk about state
% wwrite about public datasets like cityscape and synthia, kitti, commo, pascal voc
% 
\section{Motivation}
% self driving , autonomuous car
% semantic segmentation is heavy that's why simple cnn
% discussion on pop conv nets, lenet has become hello
%  Focus on single multi-lane highway without lane markings

The development of autonomous systems is capable of supporting people in daily tasks is one of the major challenges in modern computer science. Autonomous driving systems are one example that can help to reduce casualties caused by traffic accidents. Although autonomous driving systems commonly use GPS, radar, laser range finders, LIDAR and high precision environmental maps. But, LIDAR for example can be very expensive solutions \cite{verge_elon_lidar}. In this thesis we aim at cheaper solutions by using visual sensors for tasks such as object detection, navigation and manipulation. Scene classification is a requirement for many high levels of work in real-world environments for any automated smart machine and is an important aspect of computer vision. \q{Convolutional networks are at the core of most stateof-the-art computer vision solutions for a wide variety of tasks} \cite{Szegedy_2016_CVPR}. But training CNNs requires large-scale datasets. Transfer learning addresses this problem and produces a solution for small-scale datasets. So the cheaper option is to use synthetic data. We need to develop computer simulation, like a car racing game, to mimick videos taken from dashboard camera and gether synthetic data. We know that semantic annotation of different objects precisely at pixel level is humanly impossible. That is why specialized softwares like LabelImg \cite{LabelImg}, Supervisely \cite{supervise}, LabelBox \cite{labelbox}, etc are used to annotate images. But still annotating thousands of images is pretty hefty task. That is why we need to annotate the data gathered from our computer simulation automatically or very less human efforts.

% air sim is High-Fidelity, ours is low fide for simlicity


% Scene classification is a significant aspect of computer vision. Convolutional neural networks (CNNs), a development of deep learning, are a well-understood tool for image classification. But training CNNs requires large-scale datasets. Transfer learning addresses this problem and produces a solution for small-scale datasets.

% A convolutional neural network (CNN) trained on ImageNet (Deng et al., 2009) significantly outperforms the best hand crafted features on the ImageNet challenge (Russakovsky et al., 2014). But more surprisingly, the same network, when used as a generic feature extractor, is also very successful at other tasks like object detection on the PASCAL VOC dataset (Everingham et al., 2010).

% Convolutional neural networks have recently obtained astonishing performance on object classifi- cation (Krizhevsky et al., 2012) and scene classification (Zhou et al., 2014). 

% We also plot the confidence in the final per-pixel segmentation label predictions as the ratio of probabilities of second best label to the best label. As seen clearly in the Fig. 9, higher uncertainty occurs mostly at the object boundaries. We also tried variation ratio of the final class labels returned by using dropout for each pixel at test time[15] [23] but did not find it any more informative.

% The ability of an artificial neural network to perform a task is heavily influenced by the quality of its training set. If the training set contains examples taken from the full range of situations the network is expected to handle, the network will learn to perform the task accurately. But if important situations are missing during training, the network is likely to perform poorly when it later encounters the novel circumstances.
% In the domain of autonomous driving, we have shown that the connectionist architecture shown in Figure 1can quickly learn to steer by watching a person drive [2]. The network receives live input from a camera on the vehicle. The network is trained using back-propagation [3] to activate the output unit representing the driver’s current steering direction. After approximately four minutes of watching a person drive on a particular type of road, the network is able to take over for itself and drive at up to 55 miles per hour. Individual networks have been trained to drive in a wide variety of situations, including single and multi-lane roads with and without lane markings.

% Here we show that object detectors emerge from training CNNs to perform scene classification. As scenes are composed of objects, the CNN for scene classifica- tion automatically discovers meaningful objects detectors, representative of the learned scene categories. With object detectors emerging as a result of learning to recognize scenes, our work demonstrates that the same network can perform both scene recognition and object localization in a single forward-pass, without ever having been explicitly taught the notion of objects.

% Convolutional neural networks have recently obtained astonishing performance on object classification (Krizhevsky et al., 2012) and scene classification (Zhou et al., 2014). The ImageNet-CNN from Jia (2013) is trained on 1.3 million images from 1000 object categories of ImageNet (ILSVRC 2012) and achieves a top-1 accuracy of 57.4%. With the same network architecture, Places-CNN is trained on 2.4 million images from 205 scene categories of Places Database (Zhou et al., 2014), and achieves a top-1 accuracy of 50.0%. 


\section{Thesis Goal}
This thesis aims at developing a machine learning model which can differentiate between lanes in a image of single or multi-lane highway. We need a large amount of image data of traffic scenes to be used by machine learning model in order to give correct predictions. Since, gathering high-quality real world labelled data of traffic scene in abundance is not feasible, we aim to gather the data through a computer simulation. Then we need to label the data automatically. The model should be able to take image and a point coordinate as inputs and predicts the lane on which point exists. Keeping in mind that our data is in image format and we aim to classify a point in image, we need to choose an algorithm for that and implement it. In the end we need to evaluate the results and show our predictions in human understandable form.
\section{Related Work}
In this section, we will explore the research on traffic scene classification done by other computer scientists.
\subsection{Related Research: I}
\par
Convolutional Neural Network has become the conventional model for most of the Computer Vision task. In \cite{zhou2014object} the authors analyzed the inner layers of deep learning architectures. They followed a simple philosophy that objects make up a scene and if a CNN is able to do scene classification then it must be able to detect objects. They proposed that the inner layers learns the representation and is responsible for object detection. They also showed that a single Convolutional Neural Network can do both object detection/localization and scene recognition. The study also stressed on the point that although the representations learned by the layers of a deep learning network is confusing but it is substantially performs better than hand crafted features which are easy to comprehend.
\par
They discussed that there is a renowned dataset for object recognition, object localization and image classification known as ImageNet \cite{deng2009imagenet} and Places \cite{zhou2017places} for scene recognition. A convolutional neural network known as ImageNet-CNN \cite{donahue2014decaf} trained on ImageNet for object recognition achieved a good accuracy in ImageNet Challenge \cite{Russakovsky2015}. But when a network architecture same as ImageNet-CNN was trained on Places dataset for scene recognition, it astonishingly performed well and was able to do object localization through features in inner layers. Figure \ref{fig:places_cnn} shows the output of inner layers of the network trained. Hence, they concluded that a CNN can perform object localization even if it is trained to classify scenes.

\begin{figure}[H]
  \centering
  \includegraphics[scale=0.40]{images/Chapter1/places_cnn.png}
  \caption{The first images shows the recognized scene. Rest of the images show output of inner layers along with semantic tag and confidence.}
  \label{fig:places_cnn}
\end{figure}

\par
It can be helpful for our thesis because given our traffic image data we can identify the maximum number of lanes and objects like vehicles, lanes, people, traffic signs using a single CNN, without using a separate model for object detection.

\subsection{Related Research: II}
As the name \q{Training Deep Networks with Synthetic Data: Bridging the Reality Gap by Domain Randomization} suggests, this paper \cite{Training_Deep_Networks_with_Synthetic_Data}, demonstrates object detection being done a deep neural network trained on synthetic or simulated data. The author explains that not only good quality real-world labelled data in abundace is very expensive, generating large amount of photorealistic synthetic data can also become expensive. In order to cover different situations in real world, they used an approach called \q{domain randomization} \cite{domain_randomization}. Domain randomization is a process to generate complex environments in simulation with the help of different properties for example, lighting, material textures, camera viewpoints, position of objects, etc. The randomized data generated by this process can be used to train neural networks to learn important features of the object of interest. They used Faster R-CNN network with Inception ResNet V2 as the feature extractor to pre-train the network on a high-fidelity Virtual KITTI or VKITTI dataset \cite{VKITTI} and domain randomized (DR) dataset. And later fine-tune the network with real-world data. The authors proposed that the combination of using low-cost synthetic domain randomized data for pre-training and real images for fine-tuning the network is more effective than the networks trained on expensive photorealistic datasets and real data alone. They evaluated this approach by detecting cars on the KITTI dataset \cite{KITTI}. Figure \ref{dr} shows the car detection by both of the approaches.

\begin{figure}[H]
  \centering
  \includegraphics[scale=0.34]{images/Chapter1/dr.png}
  \caption{Car localization on KITTI images using Faster-RCNN trained only on synthetic data, either on the VKITTI dataset or Domain randomized dataset.}
  \label{fig:dr}
\end{figure}

\subsection{Related Research: III}
The paper \cite{omer2010automatic} analyzes the classification of road surface condition during winters using images captured from a camera mounted on a vehicle. This paper is relatable to our thesis because we are also using machine learning approach on image classification of traffic scene and we also have the same camera angle to capture image. The difference is that we are simulating the scene instead of taking images in real-world. The paper also points us to consider the changes in road conditions due to change in weather, lighting and shadows.
\par
The authors emphasize that weather conditions such as heavy snowfall can drasticly change the road conditions. Hence, a system to monitor road conditions can help in driving decisions. They used feature vectors such as RGB features and gradients to train Scalar Vector Machine (SVM) for classifying road surface conditions such as snow covered and bare.

\section{Thesis Structure}
In the following chapter, the field of Machine Learning is thoroughly examined, its history, basic concepts and related topics such as Artificial Neural Networks \cite{ann} and Convolution Neural Networks are discussed. Chapter \ref{chap:3} will comprehensively explore datasets that are being used to support this thesis. It will explain how we generated data through a computer simulation and why we needed a simulation to do that. We will show what kind of data is gathered, overview of data distribution and how we preprocessed it to be used by our machine learning model. In the next Chapter \ref{chap:4}, we will talk about the tools and technologies used for data gathering, implementation, evaluation and visualization of our solution. We will also discuss the convolution neural network we are using and how we tweaked it to fit our needs. In Chapter \ref{chap:5}, the experiments that were carried out and their results will be described. All those results will be further evaluated by some metrics and graphs. We will also show some visualiztions making our predictions easier to understand by humans by detecting patterns and outliers. In the last Chapter \ref{chap:6}, we would review all the research we have done in order to explain what we have learnt from the findings and whether or not this study is successful in solving the mentioned problem. Finally, there will be discussion of future work, followed by all tasks which were planned, but not completed because of time limits.