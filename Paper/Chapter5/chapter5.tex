\par
In this chapter, we will see and discuss the implementation of different proposed solutions for the document classification problem. We will discuss both the Convolutional Neural Networks, based on textual input and images, trained for the problem. As we mentioned in the previous chapter that we used transfer learning for one of our ConvNet model so we will also discuss the reason for choosing this methodology. Furthermore, based on these methodologies, we have carried out some experiments on our dataset, which we will also evaluate and compare different solutions. Along with that, we will briefly discuss the technology stack and evaluation metrics used in the solution. We broke the implementation chapter into different sections respective to the provided solution and will see each solution one by one.
\section{Technology Stack}
\par
Before diving deep into the implementation of different solutions for document classification, let's go through some of the major tools and technologies which we used in different solutions.
\subsection{Python}
Python is one of the most popular and powerful open-source backend programming language. The main reason for its popularity is easy to use syntax. We can do wonders by writing very few lines of code. Also, python is one of the most used language in the domain of machine learning due to its enormous support and libraries for machine learning and deep learning such as Tensorflow, Scikit-learn, PyTorch, Numpy, Keras, etc.
\subsection{Tensorflow}
Tensorflow is an open-source machine learning library developed by Google and released as an open-source project in 2105 for implementing machine learning algorithms. It offers high-level APIs for the implementation of machine learning algorithms so that we don't have to go really deep for writing complex code to develop a neural network or to even configure or program a neuron. It also provides both C++ and Python APIs that makes it easier to work on and has relatively faster compilation time than other Deep Learning libraries. Tensorflow supports both CPU's and GPU's computing devices.
\subsection{Keras}
Keras is a deep learning TensorFlow library to implement deep neural networks very quickly. Why most of the developers are switching to kaggle because it is providing high-level APIs of TensorFlow implementation to implement and configure neural networks. You can use Kaggle Sequential or Functional API to build a neural network from scratch or either use transfer learning models provided as an API by Keras. It provides multiple functions to configure the neural network model like add, fit, compile, evaluate, predict, etc.
\subsection{Scikit-learn}
Scikit learn is another very powerful machine learning library used by the developers to implement machine learning algorithms. It contains many tools for supervised and unsupervised machine learning and statistical modeling. It is built on SciPy and primarily used for data modeling.
\subsection{Kaggle}
Kaggle is an online free platform for data scientists to create, implement and search machine learning and data science solutions. It provides several features like Datasets where you can search and use or even download the datasets which are published publicly. Most of the machine learning developers use kaggle to find the dataset for their machine learning problems. Along with that, it also provides a development environment in the form of Jupyter Notebook called Kaggle Kernels. This notebook can be used to run python or R programming language. The reason for its popularity is that it provides very high-level computing power specifically GPU computing and currently allowing to upload up to 10GB of data free.
\subsection{Pytesseract}
Python-tesseract is an optical character recognition (OCR) tool for python. It is primarily used for reading or extracting text from images. Python-tesseract is a wrapper for Google's Tesseract-OCR Engine.
\section{Evaluation Metrics}
In machine learning projects, after training the model for a problem, it is very necessary to check and evaluate its performance before using it in any live system. For this purpose, there are various metrics and techniques to measure performance. In this section, we will describe a few of the techniques for model's performance evaluation
\subsection{Model Accuracy}
Whenever we have to check how well our model has trained, our first step is to check the final average classification accuracy of the model which is shown in percentage indicating the confidence score and how well the model has learned from the training data. The formula for calculating accuracy is to divide the number of correct predictions during training out of the total number of predictions. When we compile the model after passing the data and setting parameters, it shows the accuracy along with the loss value for each epoch and after completing all the epochs it returns the average classification accuracy of the model.
\subsection{Loss}
Through loss value, we evaluate how well our training algorithm modeling the input dataset.  If the predictions are not as expected, then it gives a higher number. If the predictions are good and as expected, then the output will be a smaller number. This output in the form of a number and is called loss value which indicates whether our training model is well trained for the given dataset or not.  If the loss value is within the threshold then our model is fine and good to go but in case the loss value is very high, then there are some additional steps need to perform to minimize the loss value.  In the case of higher loss values, we optimize or tune our weights for the model. Here, weights are the numerical value which are the parameters set for our model.
\subsection{Confusion Matrix}
A confusion matrix is an evaluation metric to check the classification performance of a trained machine learning model. Normally, after training the model with training data, we use test data to check how well our model is working on the data which was not present during training. After that, we create a confusion matrix to check the performance which indicates the number of true predictions. Basically, it returns a matrix with n number of rows and columns where n is the number of classes for which the model is trained. Please see the Figure \ref{cm} to see a result of a confusion matrix.
\begin{figure}[H]
\centering
\includegraphics[scale=0.7]{images/Chapter5/cm.png}
\caption{Illustration of a Confusion Matrix \cite{conf_matrix}}
\label{cm}
\end{figure}
\par
\section{Implementation \& Evaluation for Solution I}
As mentioned earlier, this solution is based on a custom algorithm which is accepting textual information and apply some scoring function to determine the type of the document. Let's discuss the solution in detail and evaluate the performance.
\subsection{Proposed solution}
This solution starts with creating the dictionary of relevant words. Although, we are using some metrics to generate the dictionary by extracting text from pdf but still it requires some manual filtration of garbage words. Later on, we use this dictionary to filter the garbage data while text extraction and analysis anywhere. After generating the dictionary, we extract the text from pdfs and then categorize the text by document types. Since, this solution is not using any machine learning techniques, so we are not using high amount of data to categorize the data. Please find below the Figure \ref{text_cat_data} to see the data after applying text categorization. 
\begin{figure}[H]
\centering
\includegraphics[scale=0.9]{images/Chapter5/sol_1/sol1-data.png}
\caption{Text categorization data by Document Type}
\label{text_cat_data}
\end{figure}
\par
This categorization is further used to generate class words and corpus. As seen in the above figure  this list contains the sentences belongs to a particular document type but class words contains the list of words by each document type. Please see the Figure \ref{class_words} for the reference. On the other hand, the corpus is something which is independent of categorization and contain the list of words with its frequency of occurrences throughout the data or in other terms the commonality of each word. Please see the Figure \ref{corpus} for the reference.
\begin{figure}[H]
  \centering
  \begin{minipage}[b]{0.4\textwidth}
    \includegraphics[scale=0.9]{images/Chapter5/sol_1/class_words.png}
    \caption{Class words data by Document Type}
    \label{class_words}
  \end{minipage}
  \hfill
  \begin{minipage}[b]{0.4\textwidth}
    \includegraphics[scale=0.9]{images/Chapter5/sol_1/corpus_words.png}
    \caption{Corpus reflecting words commonality}
    \label{corpus}
  \end{minipage}
\end{figure}

\par
We are using class words and corpus in scoring function for the solution. Now, when we are done with lists of words with weight and by document types, we can do the classification. What is happening in text classification in this solution is that we extract the text from the document and pass it to our classification function which analyzes the textual information and calculates the score for each document type on the basis of this text and ultimately returns the class with the highest score. The classification function is using another method that is responsible for calculating the score for any document type. Please see Listing \ref{score_func} to see the implementation of calculating the class score. Here we are using a formula and corpus and class words for apply scoring and treat each word with relative weight.
\begin{listing}
% \inputminted[frame=lines,framesep=2mm,baselinestretch=1.2,fontsize=\scriptsize,linenos]{python}{Chapter5/classification_func.py}
\caption{Function to calculate class score}
\label{score_func}
\end{listing}
\subsection{Evaluation}
For the evaluation of performance, we used another generated dataset. We applied the above methodology to classify the document type by calculating the word frequency score for each type and found out that it is working fine with full accuracy and zero error rate. Please find below the Figure \ref{sol1_eval} to see evaluation results.
\begin{figure}[H]
\centering
\includegraphics[scale=0.7]{images/Chapter5/sol_1/sol1_eval2.png}
\caption{Evaluation of solution by Document Type}
\label{sol1_eval}
\end{figure}
\section{Implementation \& Evaluation for Solution II}
In this solution, we developed and trained a ConvNet by using Keras Sequential API. We are using textual content as the source of input to train the model. In the further sections, we will describe the neural network architecture proposed for training the model and its evaluation which is carried out on testing data.
\subsection{Proposed Neural Network Architecture}
The neural network developed for this solution is trained to predict the document types of tax-related documents. The network accepts the textual data in the form of vector input for training and is consists of 3 layers. The number of layers decided by considering the complexity of the problem in this solution. Please see Figure \ref{sol2_network_details} to know the details of the neural network used. Out of 3 layers, there are two core layers each consists of 50 nodes, where the first layer is using \q{ReLU} as an activation function, followed by a final output layer with nodes equal to the number of document types(classes) we are using. The output layer is using \q{Sigmoid} as an activation function for the prediction of the document type.
\begin{figure}[H]
\centering
\includegraphics[scale=0.7]{images/Chapter5/sol_2/network_detail_sol2.png}
\caption{Details of Neural Network used for Solution II}
\label{sol2_network_details}
\end{figure}
\par
The idea behind the implementation is to use the textual information extracted from the documents. Here, documents can be in the form of pdfs or images. After the text extraction, we apply some data processing steps where we filter the irrelevant and garbage words from the text. Furthermore, the text extracted from a single document is further broke down into small strings and applied word shuffling to introduce the distinguishing factor between the input text records. After applying all the above things, we have now a list of string where each string consists of multiple keywords related to a specific document type. Later on, we apply the bag of word approach to convert this raw text into an integer vector array so that it can be used to train the model. We used Keras Tokenizer API \cite{keras_tokenizer} to convert the text into a vector array. The Tokenizer API provides different modes to convert text to vectors. We tried different modes and found that \q{Frequency} mode works best for our problem. So, after applying the Tokenizer API, we have the vector array which we use to train our model.
\subsection{Training the Model}
At this stage, after processing our data, we are training our neural network model which is supposed to predict the document type of tax documents. Here, I would mention that we are using only three types of documents with limited formats because of the lack of data and this trained model will recognize only one document type at a time instead of giving confidence score for each document type. The parameters used to train the model are very few and needed a little bit tweaking by hit and trial to achieve the good results. Please see the below List \ref{hp_md_sol2} to know the data and parameters used to train the model.
\begin{table}[H]
\centering
\begin{tabular}{l  l  l}
 & Types/Parameters & Value \\
\hline
\\ Dataset & Lohnsteuerbescheinigung(Income Tax Statement) & 12800 \\
      & Gehaltsabrechnung (Salary slips) & 7202 \\
      & Rechnung (General Receipts) & 3653  \\\\
      
\hline
\\ Model & Layers & 3 \\
    & Activation Function (Output layer) & Sigmoid \\
    & Epochs & 30 \\
    & Trained for number of classes & 3 \\\\
\end{tabular}
\caption{Parameters setting for model training}
\label{hp_md_sol2}
\end{table}
\par
In the above list, the dataset values reflect the number of records generated by extracted text from documents and break down the text into further small strings. Why the values are not equal for each document type is because the General receipts do not contain as much text as the income tax statement and salary slips.
\subsection{Evaluation}
In this step, after training the model, it's time to evaluate the performance that how well the model is trained and how well it has generalized the training data. To find out this, we first check the final accuracy of the model and the final loss. Furthermore, to evaluate the model further, we used the test data and checked the predictions on it.
\subsubsection{Final Accuracy \& Loss}
\begin{figure}[H]
\centering
\includegraphics[scale=0.7]{images/Chapter5/sol_2/acc_graph.png}
\caption{Final Accuracy of the trained model for Solution II}
\label{acc_sol2}
\end{figure}
\par
We trained our neural network with 30 epochs and got the final average accuracy of 100\%. If we see the Figure \ref{acc_sol2}, we will see that we are getting a high accuracy of around 96\% for training data from the very first epoch and after that, the accuracy remains constant of 100\% for the rest of epochs however for test data we are getting the 100\% accuracy since the beginning. Although this may indicate a scenario of overfitting but, it needs to understand the nature of data at this stage. In our data, each input record contains more or less same kind of data with a little bit of variation for each document type which makes this problem very simple to understand for the model.
\begin{figure}[H]
\centering
\includegraphics[scale=0.7]{images/Chapter5/sol_2/loss_graph.png}
\caption{Final Loss of the trained model  for Solution II}
\label{loss_sol2}
\end{figure}
\par
Whereas, if we see the graph of loss value in Figure \ref{loss_sol2} for the model then again, we are getting a very low final loss value of 0.000404. The loss value needs to be as low as possible to have a well-trained model. For the training data, the loss value starts at 0.4 on the first epoch and then decreased to almost 0 after that whereas the test data started with the low loss value from the beginning.
\subsubsection{Confusion Matrix}
\begin{figure}[H]
\centering
\includegraphics[scale=0.7]{images/Chapter5/sol_2/cm-sol1.png}
\caption{Confusion Matrix for 3 Document types  for Solution II}
\label{cm_sol2}
\end{figure}
\par
After training the model, we evaluated our model by making predictions on the test data which is not present during the training and is totally new for the model. Here, the model again achieved the accuracy of 100\% by placing the correct labels to each prediction. To find out the accuracy, we can refer to the Figure \ref{cm_sol2} which reflects the confusion matrix of the model for the predictions on testing data. Here, we used a total of 4731 records for the testing and as you can see the matrix all the records predicted correctly.
\subsubsection{Classification Report}
\begin{figure}[H]
\centering
\includegraphics[scale=0.9]{images/Chapter5/sol_2/classification_report.png}
\caption{Classification Report for 3 Document types  for Solution II}
\label{cr_sol2}
\end{figure}
\par
The classification report also verifies the results of the confusion matrix that all the document types achieved the f1-score of 100. Also, the weighted average and accuracy of all of them showing the good numbers.
\section{Implementation \& Evaluation for Solution III}
In the third solution which is our main solution, unlike the previous solution, we are leveraging the service of transfer learning using Keras to develop and train our ConvNet model instead of creating the model from scratch. Also, the form of input is changed, as in this solution we are not relying on the textual content inside the document but instead we are giving visual information of document for the training of model by converting the pdfs into images. In further sections, we will describe the network proposed for this solution along with the different experiments carried out and its evaluation.
\subsection{Proposed Neural Network Architecture}
\begin{figure}[H]
\centering
\includegraphics[scale=0.9]{images/Chapter5/sol_3/sol3_arch.png}
\caption{Network Architecture used for Solution III }
\label{network_arch_sol3}
\end{figure}
\par
The neural network used in this solution is totally different from the network used in Solution II. Here, we used the above VGG19 architecture to develop the neural network which was again designed to predict the tax-related documents. This network accepts visual data in the form of images exactly of size 224x224 pixel as per the requirement of the VGG19 model for training. The network consists of a total of 19 layers and contains convolutional layers of filter size 3x3 and using ReLU for activation function. On top of the network, two fully connected layers of 1024 nodes using ReLU as an activation function is attached. In the end, there is an output layer that uses Softmax for activation function and has nodes equal to the number of classes which in our case is 3 and is responsible for predicting the document types.
\newline
\par
Before passing the data to the model for training, we applied some pre-processing to image dataset in the form of image augmentation. Various parameters are used to transform the images into different shapes. In our case, it was very important and helpful because of the nature of data and also due to the lack of data. The transformation involves rotation of images, zooming in, horizontal and vertical flips, height and width shift, etc. Please see Figure \ref{aug_data} in Chapter \ref{chap:3} to see the effects after augmentation.
\subsection{Training the Model}
After the pre-processing of images, our dataset is ready for the model training. Again, the purpose of this model is the same as the previous model in Solution II that is we are training this model to determine one tax document type at a time unlike the multi-label classification and the classes are also the same. For training the model, we carried out different experiments by changing the hyperparameters and augmentation parameters of the model to see the results. In the below List \ref{hp_mdl_1}, you can find out the data and parameters used to train the model.
\begin{table}[H]
\centering
\begin{tabular}{l  l  l}
 & Types/Hyperparameter & Value \\
\hline
\\ Dataset & Lohnsteuerbescheinigung(Income Tax Statement) & 4000 \\
      & Gehaltsabrechnung (Salary slips) & 4000 \\
      & Rechnung (General Receipts) & 4200  \\\\
\hline
\\ Model & Input Size & 224x224 \\
      & Batch Size & 64 \\
      & Optimizer & SGD \\
      & Learning Rate & 0.001 \\
      & Momentum & 0.9 \\
      & Epochs & 50 \\\\
\hline
\\ Regularization & Early Stopping & Yes \\ \\
\hline
\\ Preprocessing & Centering & Yes \\ 
                 & Re-size   & Yes, 224x224 \\ \\
\hline
\\ Augmentation & Rotation Range & \ang{10}  \\ 
                & Re-scale Image & multiply pixels by 1/255.0 \\ 
                & Width Shift Range & 0.1 \\ 
                & Height Shift Range & 0.1 \\ 
                & Zoom Range & 0.2 \\ 
                & Horizontal Flip & Yes \\ 
                & Vertical Flip & No \\ 
                & Brightness Range & No \\ \\
\hline
\end{tabular}
\caption{Dataset and Hyperparameters for the model}
\label{hp_mdl_1}
\end{table}
\par
In this model training, we are using the image dataset in the above-mentioned quantity for each document type. As mentioned, we used the VGG19 model for our model, so we used Keras to create our model and initialized it with the ImageNet database. The reason to initialize the model with ImageNet is that ImageNet contains the generic data of more than 1000 categories so by using ImageNet we can leverage the training of core layers already trained on the ImageNet dataset. On top of that, we applied our newly added two fully connected layers and output layer for the final prediction of the documents.
\subsection{Evaluation}
Once the model is trained, now we can evaluate the performance of the trained model. In this solution, we have used the same evaluation metrics that we used in Solution II. After the training, we checked the final accuracy and loss of the model which we will see in the next section. Along with that, we checked the predictions on test data and based on that we generated the confusion matrix and classification report for our model generalization.
\subsubsection{Final Accuracy \& Loss}
\begin{figure}[H]
\centering
\includegraphics[scale=0.8]{images/Chapter5/sol_3/ac3_sol3.png}
\caption{Final Accuracy of the trained model for Solution III }
\label{acc_sol3}
\end{figure}
\par
We trained our neural network model with 50 epoch and achieved the final average classification accuracy of 100\%. If we compare the accuracy of this solution to Solution II, we can clearly see from Figure \ref{acc_sol3} that the accuracy is increasing gradually till the final epochs, unlike Solution II where we achieved the accuracy of 100\% right after the first epoch. Here, on training data, the accuracy started with 32 percent with the first epoch and saw the increase of 5-10 percent on average in each epoch while on validation data, the accuracy starts a little bit lower than the training data but in the end, it achieves the same accuracy as of on training data. Overall the graph is indicating very good results.
\begin{figure}[H]
\centering
\includegraphics[scale=0.8]{images/Chapter5/sol_3/loss3_sol3.png}
\caption{Final Loss of the trained model for Solution III}
\label{loss_sol3}
\end{figure}
\par
If we talk about the training and validation loss, in Figure \ref{loss_sol3} the graph is pretty good and reflects the gradual decrease in the loss values of our model. On training data, the loss value started around 1.2 on the first epoch and then decreases gradually and by the final epoch, the loss value was around 0.02. Also, it should be noted that there is no crunch in the loss value means the loss value didn't increase throughout the training. On the validation data, the loss value started at 1.1 and in the end, matches the values of training loss.
\subsubsection{Confusion Matrix}
\begin{figure}[H]
\centering
\includegraphics[scale=0.8]{images/Chapter5/sol_3/cm3_sol3.png}
\caption{Confusion Matrix for 3 Document types for Solution III }
\label{cm_sol3}
\end{figure}
\par
We also evaluated the performance of our model by checking how well the model is predicting on testing data which were not used during the training and found out that the model is predicting accurately for each class of our document types by creating a confusion matrix and achieves an accuracy of 100\% on average. This indicates the generalization of our model. For the testing, we used a total of 200 images for all types of documents. Please see Figure \ref{pred_sol3} for seeing the prediction results of our model on the testing data.
\begin{figure}[H]
\centering
\includegraphics[scale=0.9]{images/Chapter5/sol_3/predictions_sol3.PNG}
\caption{Model Predictions of labels on testing data }
\label{pred_sol3}
\end{figure}
\subsubsection{Classification Report}
\begin{figure}[H]
\centering
\includegraphics[scale=0.8]{images/Chapter5/sol_3/cr3_sol3.png}
\caption{Classification Report for 3 Document types for Solution III }
\label{cr_sol3}
\end{figure}
\par
From the classification report, it also verifies the results of the confusion matrix that all the document types achieved the f1-score of 100. Also, the weighted average and accuracy of all of them showing the good numbers.